---
title: "PhenoCrops"
author: "Grupo 5"
format: html
editor: visual
---

```{r}
integrantes <- c("Campos Troyes, Edgar Orlando"
                 , "Vasquez Guevara, Jheferson Yoel"
                 , "Tapia Chavarri. Britney Emeli"
                 , "Fernandez Rojas, Jonatan Dali"
                 , "Gomez Cruz, Kevin Orlando"
                 , "Yantec Ocampo, Daemar Enrique")
participacion <- c("100%", "90%", "100%", "%", "%", "%")

tabla <- data.frame(Integrantes = integrantes, Participacion = participacion)

tabla
```

# Librerias

```{r}
library(googledrive)
library(ggplot2)
library(tidyr)
library(googlesheets4)
library(dplyr)
library(lme4)
```

## Fenotipado en ImageJ.exe

```{r}
url <- "https://docs.google.com/spreadsheets/d/1XddbJV_9Ru2_ns5UJX_xJ7c7wQ9g_u5Jge-AJNpdVdQ/edit?pli=1&gid=0#gid=0"

gs <- url %>%
  as_sheets_id()

ImageJ <- gs %>%
  range_read(sheet = "ImageJ")

View(ImageJ)
```

## Fenotipado en Ela.exe

```{r}
url <- "https://docs.google.com/spreadsheets/d/1XddbJV_9Ru2_ns5UJX_xJ7c7wQ9g_u5Jge-AJNpdVdQ/edit?pli=1&gid=0#gid=0"

gs <- url %>%
  as_sheets_id()

ela <- gs %>%
  range_read(sheet = "ela")

View(ela)
```

### Comparación de áreas obtenidad en las dos aplicaciones

```{r}
url <- "https://docs.google.com/spreadsheets/d/1XddbJV_9Ru2_ns5UJX_xJ7c7wQ9g_u5Jge-AJNpdVdQ/edit?pli=1&gid=0#gid=0"

gs <- url %>%
  as_sheets_id()

todo <- gs %>%
  range_read(sheet = "todo")

View(todo)
```

### Regresión lineal de las áreas obtenidas

```{r}

modelo <- lm(`Area ImageJ` ~ `Area ela`, data = todo)
summary(modelo)

muestras <- todo$nombre

plot(
  x = 1:nrow(todo),
  y = todo$`Area ela`,
  pch = 19,          
  col = "blue",
  xlab = "Hojas",
  ylab = "Área (cm²)",
  main = "Comparación de áreas: ela vs ImageJ",
  xaxt = "n"       
)

points(
  x = 1:nrow(todo),
  y = todo$`Area ImageJ`,
  pch = 19,
  col = "black"
)

axis(
  side = 1,
  at = 1:nrow(todo),
  labels = muestras,
  las = 2        
)

legend(
  "topleft",
  legend = c("Área ela", "Área ImageJ"),
  col = c("blue", "black"),
  pch = 19
)

abline(modelo, col = "red", lwd = 2)

coef(modelo)

summary(modelo)$r.squared


```

Los resultados muestran que las mediciones de ELA son más variables y, en varios casos, más altas que las obtenidas con ImageJ, que se mantiene más estable. La regresión lineal indica una relación prácticamente inexistente entre ambos métodos, reflejada en una pendiente cercana a cero. El bajo R² confirma que las mediciones de ELA no predicen adecuadamente las de ImageJ. En conjunto, ambos métodos no muestran concordancia fuerte y producen patrones de medición distintos.

# Entrenamineto de IA

## Desarrollo del proceso

Las imágenes fueron exportadas de labelstudio, de la práctica anterior. ###Preparación de datos Primero se cargaron las imágenes y archivos de anotación generados en formato YOLO. Luego, el scrip los clasificó en "data/images" y "data/labels".

### Aumentación masiva del dataset (20x)

Esto se hizo para mejorar el rendimiento del modelo, se generó 20 nuevas versionas por cada imagen original. Dichas transformaciones incluyen rotación aleatoria hasta 40°, ajuste del brillo y contraste, escalado proporcional. Y los resultados los almacena en "argumented/images" y "argumented/labels".

###Conversión de anotaciones YOLO a COCO

Como para el entrenamiento se empleó Mask R-CNN, la mejor opción fue hacer con Detectron2 y para ello se requierían los archivos en formato COCO Instance Segmentation. El scrip procesó cada archivo YOLO aumentado y al final se almacenó en "dataset_coco.json", que contiene las entregas generadas para el entrenamiento y además es compatible con Detectron2.

### Correción de rutas y etiquetas

Como Detectron2 necesita de rutas específicas, se extrajo el nombre original del archivo y se reemplazó cualquier ruta por "data/images/"nombre".jpg", y esto reeescribe toda la sección de imágenes en el JSON.

### Entrenamiento del modelo

Se usó Mask R-CNN con backbone ResNet-50 FPN, un modelo de segmentación robusto y estándar. Aquí se configuró las clases (Hoja y Escala), Iteraciones de entrenamiento (3000), Bath size (2 imágenes por iteración, adaptado para GPU de Colab), Learning rat (0.00025), Checkpoints (cada 500 iteraciones), Dataset ("leaf_train"). Al final dió un archivo llamado "model_final.pth" que se usó para la interencia y la aplicaicón iinteractiva.

### Inferencia sobre imágenes nuevas

Se carga el modelo entrenado "model_final.pth", se selecciona una imagen nueva, Detectron2 realzia la segmentación de la hoja, segmentación del objeto de referencia ("Escala") y genera un dibujo visual de máscaras.

### Desarrollo de la aplicación interactiva (Gradio)

Aquí se construye una interfaz completa en Gradio diseñada para permitir mediciones reales en fotografías de hojas usando una escala física. La funcionalidad de la aplicación: - Detección automática: se carga una imagen y la app detecta la hoja, la escala y muestra índices y áreas. - Medición manual mediante clics: esto se incluyó para el caso de imágenes nuevas, que tengan una escala diferente a las uasadas en el entrenamiento (Se usó una regla). - Visualización final: Se genera un reporte con todas las medidas calculadas, hoja segmentada y sus límites.

El resultado final es el área en pixleles y cm², largo y ancho en pixeles y cm, perímetro en píxeles y cm, centímetro por píxeles calculados, información del objeto de referencia (escala detectada).

# Anexos

-   Imágenes usadas: https://drive.google.com/drive/folders/1pv3_1awIPfNOG9ZR2jCWH8xkxYaGVREK

-   Imágenes de ela.exe: https://drive.google.com/drive/folders/1SRlYCb1ZVEdITpXi_CzIaZVbdB3QEvHP?usp=sharing

-   Enlace de Colab con el código: https://colab.research.google.com/drive/15mq6eNmU5vNbp9s4cm0jY-aoPjVtxKOf?usp=sharing

-   Se adjunta el la carpeta comprimida de la interfaz creada.

-   Para que la interfaz tenga exito es necesario ejecutar desde Windows PowerShell como administrador el siguiente código:\
    cd C:\EOCT\_Analyzer

    venv\Scripts\activate

    python app.py

-   Enlace de la interfaz: <http://localhost:7860/> y <http://127.0.0.1:7860>
